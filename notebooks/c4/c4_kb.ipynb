{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High-level Approach\n",
    "- Crawl and scrape C4â€™s website and docs using Scrapy lib\n",
    "- Convert the html content to markdown format so that the model can better understand the context\n",
    "- Use LangChain lib to do the following:-\n",
    "    - Split the markdown header-separated sections into semantic chunks\n",
    "    - Embed and store the semantic chunks in Weaviate vector db\n",
    "    - Use the retrieval augmented functionality to answer the question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install all the third-party packages\n",
    "\n",
    "!pip install 'langchain[llms]'\n",
    "!pip install Scrapy\n",
    "!pip install html2text\n",
    "!pip install lxml\n",
    "!pip install python-dotenv\n",
    "!pip install \"unstructured[all-docs]\"\n",
    "!pip install tiktoken\n",
    "!pip install faiss-cpu \n",
    "!pip install GitPython\n",
    "!pip install notebook\n",
    "!pip install chromadb\n",
    "!pip install pandas\n",
    "!pip install rank_bm25\n",
    "!pip install weaviate-client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from dotenv import load_dotenv\n",
    "from IPython.display import display, Markdown, Latex\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import getpass\n",
    "import os\n",
    "\n",
    "OPENAI_API_KEY = os.environ['OPENAI_API_KEY']\n",
    "\n",
    "WEAVIATE_URL = os.environ['WEAVIATE_URL']\n",
    "WEAVIATE_API_KEY = os.environ['WEAVIATE_API_KEY']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths to the data\n",
    "\n",
    "BASE_DATA_DIR = \"./__data__\"\n",
    "C4_WEBSITE_STORAGE_DIR = os.path.join(BASE_DATA_DIR, \"c4_website\")\n",
    "C4_GH_DOCS_STORAGE_DIR = os.path.join(BASE_DATA_DIR, \"c4_gh_docs\")\n",
    "\n",
    "if not os.path.exists(BASE_DATA_DIR):\n",
    "    os.makedirs(BASE_DATA_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download Knowledge Base data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Crawling and Scraping C4 website using Scrapy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import scrapy\n",
    "import html2text\n",
    "import lxml.html\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "from scrapy.crawler import CrawlerRunner\n",
    "from scrapy.utils.project import get_project_settings\n",
    "from twisted.internet import reactor\n",
    "\n",
    "\n",
    "class GenericSpider(scrapy.Spider):\n",
    "    name = 'generic'\n",
    "\n",
    "    def __init__(self, domain='', storage_dir='.', *args, **kwargs):\n",
    "        super(GenericSpider, self).__init__(*args, **kwargs)\n",
    "        self.allowed_domains = [domain]\n",
    "        self.start_urls = [f'http://{domain}/']\n",
    "        self.storage_dir = storage_dir\n",
    "    \n",
    "    def parse(self, response):\n",
    "        # Remove unwanted elements using lxml\n",
    "        tree = lxml.html.fromstring(response.text)\n",
    "        \n",
    "        # Remove non-text related tags\n",
    "        for unwanted in tree.xpath('//script|//img|//video|//audio|//iframe|//object|//embed|//canvas|//svg|//link|//source|//track|//map|//area'):\n",
    "            unwanted.drop_tree()\n",
    "\n",
    "        cleaned_html = lxml.html.tostring(tree).decode('utf-8')\n",
    "\n",
    "        # Convert HTML to Markdown\n",
    "        converter = html2text.HTML2Text()\n",
    "        markdown_text = converter.handle(cleaned_html)\n",
    "\n",
    "        # Save to a markdown file in the specified directory\n",
    "        if not os.path.exists(self.storage_dir):\n",
    "            os.makedirs(self.storage_dir)\n",
    "\n",
    "        url = response.url\n",
    "        page_name = response.url.split(\"/\")[-1] if response.url.split(\"/\")[-1] else \"index\"\n",
    "\n",
    "        filename = os.path.join(self.storage_dir, f'{page_name}.json')\n",
    "\n",
    "        with open(filename, 'w') as f:\n",
    "            # Store the URL and markdown text in JSON format\n",
    "            json.dump({'url': url, 'md_content': markdown_text}, f)\n",
    "\n",
    "        # Recursively follow relative links to other pages on the same domain\n",
    "        for href in response.css('a::attr(href)').getall():\n",
    "            url = response.urljoin(href)\n",
    "            if urlparse(url).netloc in self.allowed_domains:\n",
    "                yield scrapy.Request(url, self.parse)\n",
    "\n",
    "\n",
    "\n",
    "settings = get_project_settings()\n",
    "runner = CrawlerRunner(settings)\n",
    "runner.crawl(GenericSpider, domain=\"code4rena.com\", storage_dir=C4_WEBSITE_STORAGE_DIR)\n",
    "d = runner.join()\n",
    "d.addBoth(lambda _: reactor.stop())\n",
    "reactor.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Get docs from Github Repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from git import Repo\n",
    "\n",
    "repo = Repo.clone_from(\n",
    "    \"https://github.com/code-423n4/docs\", to_path=C4_GH_DOCS_STORAGE_DIR\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Retrieval Augmented Generation using LangChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load locally saved scraped data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "def load_json_files(dir):\n",
    "    loader = DirectoryLoader(dir, loader_cls=TextLoader)\n",
    "    documents = loader.load()\n",
    "    for d in documents:\n",
    "        page_content_dict = json.loads(d.page_content)\n",
    "        d.page_content = page_content_dict['md_content']\n",
    "        d.metadata['url'] = page_content_dict['url']\n",
    "    return documents\n",
    "\n",
    "c4_website_data_list = load_json_files(C4_WEBSITE_STORAGE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import DirectoryLoader\n",
    "from langchain.document_loaders import TextLoader\n",
    "\n",
    "loader = DirectoryLoader(C4_GH_DOCS_STORAGE_DIR, loader_cls=TextLoader)\n",
    "c4_gh_docs_data_list = loader.load()\n",
    "\n",
    "\n",
    "for i, d in enumerate(c4_gh_docs_data_list):\n",
    "    local_path = d.metadata['source']\n",
    "\n",
    "    if \"/README.md\" in local_path:\n",
    "        # remove README.md from the path\n",
    "        local_path = local_path.replace(\"/README.md\", \"\")\n",
    "    \n",
    "    if \"/SUMMARY.md\" in local_path:\n",
    "        # remove SUMMARY.md from the path\n",
    "        local_path = local_path.replace(\"/SUMMARY.md\", \"\")\n",
    "    \n",
    "    # remove .md from the path\n",
    "    local_path = local_path.replace(\".md\", \"\")\n",
    "\n",
    "    d.metadata['url'] = f\"{local_path.replace(C4_GH_DOCS_STORAGE_DIR, 'https://docs.code4rena.com')}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Split markdown formatted data into chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import (\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    Language,\n",
    ")\n",
    "\n",
    "md_splitter = RecursiveCharacterTextSplitter.from_language(\n",
    "    language=Language.MARKDOWN, chunk_size=2000, chunk_overlap=200\n",
    ")\n",
    "\n",
    "\n",
    "website_chunks =  md_splitter.split_documents(c4_website_data_list)\n",
    "gh_docs_chunks = md_splitter.split_documents(c4_gh_docs_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "website_chunks_with_source = [d.copy(deep=True) for d in website_chunks]\n",
    "\n",
    "for i, d in enumerate(website_chunks_with_source):\n",
    "    d.metadata['source'] = f\"{i}-pl\"\n",
    "\n",
    "website_chunks_offset = len(website_chunks_with_source)\n",
    "website_chunks_offset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "gh_docs_chunks_with_source = [d.copy(deep=True) for d in gh_docs_chunks]\n",
    "\n",
    "for i, d in enumerate(gh_docs_chunks_with_source):\n",
    "    local_path = d.metadata['source']\n",
    "    d.metadata['source'] = f\"{i+website_chunks_offset}-pl\"\n",
    "\n",
    "len(gh_docs_chunks_with_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Populate Weaviate Vector DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import weaviate\n",
    "import os\n",
    "from langchain.vectorstores import Weaviate\n",
    "\n",
    "WEAVIATE_CODEARENA_INDEX_NAME = \"CodeArenaDocsV1\"\n",
    "\n",
    "weaviate_client = weaviate.Client(\n",
    "    url=WEAVIATE_URL,\n",
    "    auth_client_secret=weaviate.AuthApiKey(api_key=WEAVIATE_API_KEY),\n",
    "    additional_headers={\"X-OpenAI-Api-Key\": OPENAI_API_KEY},\n",
    ")\n",
    "weaviate = Weaviate(weaviate_client, WEAVIATE_CODEARENA_INDEX_NAME, text_key='text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = {\n",
    "    \"classes\": [\n",
    "        {\n",
    "            \"class\": WEAVIATE_CODEARENA_INDEX_NAME,\n",
    "            \"description\": \"CodeArena docs index\",\n",
    "            \"vectorizer\": \"text2vec-openai\",\n",
    "            \"moduleConfig\": {\n",
    "                \"text2vec-openai\": {\n",
    "                    \"model\": \"ada\",\n",
    "                    \"modelVersion\": \"002\",\n",
    "                    \"type\": \"text\",\n",
    "                }\n",
    "            },\n",
    "            \"properties\": [\n",
    "                {\n",
    "                    \"dataType\": [\"text\"],\n",
    "                    \"description\": \"The content of the chunk\",\n",
    "                    \"moduleConfig\": {\n",
    "                        \"text2vec-openai\": {\n",
    "                            \"skip\": False,\n",
    "                            \"vectorizePropertyName\": False,\n",
    "                        }\n",
    "                    },\n",
    "                    \"name\": 'text',\n",
    "                },\n",
    "                {\n",
    "                    \"dataType\": [\"text\"],\n",
    "                    \"description\": \"The source id of the chunk\",\n",
    "                    \"moduleConfig\": {\n",
    "                        \"text2vec-openai\": {\n",
    "                            \"skip\": True,\n",
    "                            \"vectorizePropertyName\": False,\n",
    "                        }\n",
    "                    },\n",
    "                    \"name\": 'source',\n",
    "                },\n",
    "                {\n",
    "                    \"dataType\": [\"text\"],\n",
    "                    \"description\": \"The reference url of the chunk\",\n",
    "                    \"moduleConfig\": {\n",
    "                        \"text2vec-openai\": {\n",
    "                            \"skip\": True,\n",
    "                            \"vectorizePropertyName\": False,\n",
    "                        }\n",
    "                    },\n",
    "                    \"name\": 'url',\n",
    "                },\n",
    "            ],\n",
    "        },\n",
    "    ]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "weaviate_client.schema.delete_class(WEAVIATE_CODEARENA_INDEX_NAME)\n",
    "weaviate_client.schema.create(schema)\n",
    "weaviate.add_documents(website_chunks_with_source + gh_docs_chunks_with_source)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Run test queries on the populated vector db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "query_result = weaviate_client.query\\\n",
    "    .get(WEAVIATE_CODEARENA_INDEX_NAME, [\"text\", \"source\", \"url\"])\\\n",
    "    .with_hybrid(\n",
    "        query=\"What is Scout\"\n",
    "    )\\\n",
    "    .with_limit(4)\\\n",
    "    .do()\n",
    "\n",
    "print(query_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cw3-bot-platform",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
